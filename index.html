<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Uttaran Bhattacharya</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- Additional CSS Files -->
  <link rel="stylesheet" href="assets/css/academicons-1.9.1/css/academicons.min.css" />

  <!-- =======================================================
  * Template Name: iPortfolio - v3.3.0
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

<!--  <div id="se-pre-con" class="se-pre-con"></div>-->

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/images/Uttaran_2022.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Uttaran Bhattacharya</a></h1>
        <div class="mt-3 text-light text-center">
          Research Scientist<br>
          Adobe Inc.<br>
          San Jose, CA, USA
        </div>
        <div class="social-links mt-3 text-center">
          <a href="assets/docs/Uttaran_Bhattacharya_CV.pdf" target="_blank" class="curriculum-vitae">
            <i class="ai ai-cv"></i>
          </a>
          <a href="https://github.com/UttaranB127" target="_blank" class="github">
            <i class="bx bxl-github"></i>
          </a>
          <a href="https://scholar.google.com/citations?user=xx9nrfoAAAAJ&hl=en" target="_blank" class="google-scholar">
            <i class="ai ai-google-scholar"></i>
          </a>
          <a href="https://www.linkedin.com/in/uttaran-bhattacharya/" target="_blank" class="linkedin">
            <i class="bx bxl-linkedin"></i>
          </a>
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <li>
            <a href="#about" class="nav-link scrollTo"><i class="bx bx-user"></i><span>About Me</span></a>
          </li>
          <li>
            <a href="#research" class="nav-link scrollTo">
              <i class="bx bx-book-content"></i><span>Research Interests</span>
            </a>
          </li>
          <li>
            <a href="#bibliography" class="nav-link scrollTo"><i class="bx bx-file"></i><span>Bibliography</span></a>
          </li>
          <li>
            <a href="#contact" class="nav-link scrollTo"><i class="bx bx-mail-send"></i><span>Contact</span></a>
          </li>
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title">
          <h2>About Me</h2>
          <p>Welcome to my homepage! I am a Research Scientist at <a href="https://research.adobe.com" target="_blank">Adobe Inc.</a>,
          currently working on the development and evaluation of generative AI models for multimodal content synthesis consisting of text,
          images, videos, and 3D. My general research interests span the domains of
          <a href="https://www.sciencedirect.com/topics/computer-science/affective-computing" target="_blank">
          affective computing</a> and <a href="https://paperswithcode.com/task/motion-synthesis" target="_blank">human
          motion recognition and synthesis</a>. I completed my Ph.D. in <a href="http://www.cs.umd.edu" target="_blank">Computer Science</a>
          at the <a href="https://www.umd.edu" target="_blank">University of Maryland, College Park, USA</a>, where I worked on
          <a href="https://dl.acm.org/doi/book/10.5555/AAI29396872" target="_blank">Affective Human Motion Detection and Synthesis</a>, and was advised by
          <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Prof. Dinesh Manocha</a> and
          <a href="https://www.cs.purdue.edu/homes/ab/" target="_blank">Prof. Aniket Bera</a>. Prior to this, I completed my
          Master of Engineering and then worked one year as a research associate with
          <a href="https://ee.iisc.ac.in/~venu/" target="_blank">Prof. Venu Madhav Govindu</a> at the
          <a href="https://ee.iisc.ac.in/" target="_blank">Department of Electrical Engineering</a>,
          <a href="https://www.iisc.ac.in/" target="_blank">Indian Institute of Science</a>, where I developed efficient
          and robust algorithms for robust large-scale 3D reconstructions of objects and scenes from RGB images and raw depth maps.</p>
        </div>
      </div>

      <!-- ======= Latest News Section ======= -->
      <section id="news" class="facts section-bg">
        <div class="container">

          <div class="section-title">
            <h3>Latest News</h3>
          </div>

          <div class="row no-gutters">

            <div class="col-lg-3 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
              <div class="count-box">
                <i class="bi bi-file-earmark-text"></i>
                <p>
                  <strong>August 2024</strong>
                  <br><strong>One paper</strong> at ACL 2024
                </p>
              </div>
            </div>

            <div class="col-lg-3 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
              <div class="count-box">
                <i class="bi bi-file-earmark-text"></i>
                <p>
                  <strong>June 2024</strong>
                  <br><strong>Two papers</strong> at CVPR 2024
                </p>
              </div>
            </div>

            <div class="col-lg-3 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
              <div class="count-box">
                <i class="bi bi-file-earmark-text"></i>
                <p>
                  <strong>May 2024</strong>
                  <br><strong>One paper</strong> at ICLR 2024
                </p>
              </div>
            </div>

            <div class="col-lg-3 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
              <div class="count-box">
                <i class="bi bi-award"></i>
                <p>
                  <strong>November 2022</strong>
                  <br><strong>Best paper award</strong> at ACM MIG 2022
                </p>
              </div>
            </div>

            <!-- <div class="col-lg-3 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
              <div class="count-box">
                <i class="bi bi-briefcase"></i>
                <p>
                  <strong>September 2022</strong>
                  <br>Starting at Adobe Inc. as a <strong>Research Scientist</strong>
                </p>
              </div>
            </div> -->

          </div>

        </div>
      </section><!-- End Latest News Section -->

      <!-- ======= Education and Experience Section ======= -->
      <section id="ed-exp" class="resume">
        <div class="container">

          <div class="row">
            <div class="col-lg-6" data-aos="fade-up">

              <h3 class="resume-title">Education</h3>
              <div class="resume-item">
                <h4>Ph.D. in Computer Science</h4>
                <h5>2018 - 2022</h5>
                <p><em>University of Maryland, College Park, USA</em></p>
                <p>Advisor:
                  <a href="https://www.cs.umd.edu/people/dmanocha" target="_blank">Dinesh Manocha</a>
                </p>
              </div>
              <div class="resume-item">
                <h4>M.E. in System Science and Automation</h4>
                <h5>2015 - 2017</h5>
                <p><em>Indian Institute of Science, Bengaluru, India</em></p>
                <p>Advisor:
                  <a href="http://www.ee.iisc.ac.in/people/faculty/venu/index.html" target="_blank">Venu Madhav Govindu</a>
                </p>
              </div>
              <div class="resume-item">
                <h4>B.Tech. in Computer Science and Engineering</h4>
                <h5>2011 - 2015</h5>
                <p><em>West Bengal University of Technology, Kolkata, India</em></p>
              </div>
            </div>

            <div class="col-lg-6" data-aos="fade-up" data-aos-delay="100">
              <h3 class="resume-title">Experience</h3>
              <div class="resume-item">
                <h4>Research Scientist</h4>
                <h5>September 2022 - Present</h5>
                <p><em>Adobe Inc., San Jose, CA, USA</em></p>
              </div>
              <div class="resume-item">
                <h4>Research Intern</h4>
                <h5>May 2022 - August 2022</h5>
                <p><em>Google LLC, Mountain View, CA, USA</em></p>
              </div>
              <div class="resume-item">
                <h4>Research Intern</h4>
                <h5>May 2021 - August 2021, May 2020 - March 2021</h5>
                <p><em>Adobe Inc., San Jose, CA, USA</em></p>
              </div>
              <div class="resume-item">
                <h4>Research Associate</h4>
                <h5>July 2017 - May 2018</h5>
                <p><em>Indian Institute of Science, Bengaluru, India</em></p>
              </div>
              <div class="resume-item">
                <h4>Software Engineer Intern</h4>
                <h5>June 2014 - July 2014</h5>
                <p><em>Tata Consultancy Services Innovations Lab, Kolkata, India</em></p>
              </div>
            </div>
          </div>

        </div>
      </section><!-- End Education and Experience Section -->

      <!-- ======= Invited Talks Section ======= -->
      <section id="invited-talks" class="bodysec">
        <div class="container">

          <div class="section-title">
            <h3>Invited Talks</h3>
          </div>

          <div class="row">
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-microphone"></i></div>
              <p class="description">
                <strong>Current Trends in 3D Pose and Motion Generation</strong>
                <br><em>2024-04-23 &nbsp;&nbsp; Purdue University</em>
                <br><em>2024-03-12 &nbsp;&nbsp; ACM@UMD Speaker Series</em>
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-microphone"></i></div>
              <p class="description">
                <strong>Affective Human Motion Detection and Synthesis</strong>
                <br><em>2022-11-08 &nbsp;&nbsp; University of Southern California</em>
              </p>
            </div>

          </div>

        </div>
      </section><!-- End Invited Talks Section -->

      <!-- ======= Awards and Nominations Section ======= -->
      <section id="awards" class="bodysec section-bg">
        <div class="container">

          <div class="section-title">
            <h3>Awards and Nominations</h3>
          </div>

          <div class="row">
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-medal"></i></div>
              <p class="description">
                <strong>ACM MIG 2022 Best Paper Award</strong>
                <br>"Learning Gait Emotions Using Affective and Deep Features" by
                <em>Tanmay Randhavane, Uttaran Bhattacharya, Pooja Kabra, Kyra Kapsaskis, Kurt Gray, Dinesh Manocha, and Aniket Bera</em>.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-award"></i></div>
              <p class="description">
                <strong>UMD Invention of the Year Award 2022 Nominee</strong>
                <br>"Deepfake Detection Tool" by <em>Trisha Mittal, Aniket Bera, Uttaran Bhattacharya, Rohan Chandra, and Dinesh Manocha</em>.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-award"></i></div>
              <p class="description">
                <strong>UMD Invention of the Year Award 2021 Nominee</strong>
                <br>"M3ER: Multiplicative Multimodal Emotion Recognition" by <em>Trisha Mittal, Aniket Bera, Uttaran Bhattacharya, Rohan Chandra, and Dinesh Manocha</em>.
              </p>
            </div>
              
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-award"></i></div>
              <p class="description">
                <strong>ACMMM 2021 Best Paper Award Nominee</strong>
                <br>"Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective
                Expression Learning" by <em>Uttaran Bhattacharya, Elizabeth Childs, Nicholas Rewkowski, and Dinesh Manocha</em>.
              </p>
            </div>
  
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-medal"></i></div>
              <p class="description">
                <strong>IEEE VR 2021 Best Paper Award</strong>
                <br>"Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual
                Agents" by <em>Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera,
                and Dinesh Manocha</em>.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-graduation"></i></div>
              <p class="description">
                <strong>Adobe Research Fellowship, 2021</strong>
                <br>Adobe Inc.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-medal"></i></div>
              <p class="description">
                <strong>ACM SAP 2019 Best Poster Award</strong>
                <br>"Identifying Emotions from Walking Using Affective and Deep Features" by
                <em>Tanmay Randhavane, Uttaran Bhattacharya, Aniket Bera, Kyra Kapsaskis, Kurt Gray,
                  and Dinesh Manocha</em>.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-graduation"></i></div>
              <p class="description">
                <strong>Dean's Fellowship, 2018</strong>
                <br>University of Maryland, College Park, USA.
              </p>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="icon"><i class="bx bxs-medal"></i></div>
              <p class="description">
                <strong>Outstanding Student Award, 2013</strong>
                <br>Institute of Engineering and Management under the West Bengal University of Technology,
                Kolkata, India.
              </p>
            </div>
          </div>

        </div>
      </section><!-- End Awards and Nominations Section -->

    </section><!-- End About Section -->

    <!-- ======= Research Interests Section ======= -->
    <section id="research" class="portfolio">
      <div class="container">

        <div class="section-title">
          <h2>Research Interests</h2>
        </div>
        <div class="row" data-aos="fade-up">
          <div class="col-lg-12 d-flex justify-content-center">
            <ul id="portfolio-filters">
              <li data-filter="*" class="filter-active">All</li>
              <li data-filter=".filter-mmcg">Multimodal Content Understanding</li>
              <li data-filter=".filter-affc">Affective Computing</li>
              <!-- Archived -->
              <!-- <li data-filter=".filter-ls3d">Large-Scale 3D Reconstruction</li> -->
            </ul>
          </div>
        </div>

        <div class="row portfolio-container" data-aos="fade-up" data-aos-delay="100">

          <div class="col-lg-4 col-md-6 portfolio-item filter-mmcg">
            <div class="portfolio-wrap">
              <img src="assets/images/202408_tamerd.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/researchdirections/affectivecomputing/tame_rd/" target="_blank">
                  TAME-RD: Text Assisted Replication of Image Multi-Adjustments for Reverse Designing
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-mmcg">
            <div class="portfolio-wrap">
              <img src="assets/images/202406_handiffuser.jpg" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Narasimhaswamy_HanDiffuser_Text-to-Image_Generation_With_Realistic_Hand_Appearances_CVPR_2024_paper.pdf" target="_blank">
                  HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-mmcg">
            <div class="portfolio-wrap">
              <img src="assets/images/202402_daw.jpeg" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://github.com/aneeshbhattacharya/DanceAnyWay" target="_blank">
                  DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-affc">
            <div class="portfolio-wrap">
              <img src="assets/images/202110_s2ag.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/s2ag" target="_blank">
                  Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective
                  Expression Learning
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-affc">
            <div class="portfolio-wrap">
              <img src="assets/images/202104_t2g.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/t2g" target="_blank">
                  Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-affc">
            <div class="portfolio-wrap">
              <img src="assets/images/202011_generating_emotive_gaits.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/gen_emotive_gaits" target="_blank">
                  Generating Emotive Gaits for Virtual Agents Using Affect-Based Autoregression
                </a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 portfolio-item filter-affc">
            <div class="portfolio-wrap">
              <img src="assets/images/202002_step.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/step" target="_blank">
                  STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits
                </a>
              </div>
            </div>
          </div>
          
          <!-- Archived -->
          <!-- <div class="col-lg-4 col-md-6 portfolio-item filter-affc">
            <div class="portfolio-wrap">
              <img src="assets/images/202010_taew.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://gamma.umd.edu/taew" target="_blank">
                  Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and
                  Affective Mapping
                </a>
              </div>
            </div>
          </div> -->

          <!-- <div class="col-lg-4 col-md-6 portfolio-item filter-ls3d">
            <div class="portfolio-wrap">
              <img src="assets/images/201910_effrobse3.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.pdf"
                   target="_blank">
                  Efficient and Robust Registration on the 3D Special Euclidean Group
                </a>
              </div>
            </div>
          </div> -->

          <!-- <div class="col-lg-4 col-md-6 portfolio-item filter-ls3d">
            <div class="portfolio-wrap">
              <img src="assets/images/201710_fastmvplane.png" class="img-fluid" alt="">
              <div class="portfolio-links">
                <a href="https://ieeexplore.ieee.org/abstract/document/8374609"
                   target="_blank">
                  Fast Multiview 3D Scan Registration Using Planar Structures
                </a>
              </div>
            </div>
          </div> -->

        </div>

      </div>
    </section><!-- End Research Interests Section -->

    <!-- ======= Bibliography Section ======= -->
    <section id="bibliography" class="portfolio section-bg">
      <div class="container">

        <div class="section-title">
          <h2>Bibliography</h2>
        </div>
      </div>

      <!-- ======= Refereed Publications Section ======= -->
      <section id="refereed" class="bodysec section-bg">
        <div class="container">

          <div class="section-title">
            <h3>Selected Publications</h3>
          </div>

          <div class="section-title">
            <h4>Multimodal Content Understanding</h4>
          </div>

          <div class="row">
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <!-- <ol reversed class="paper-list"> -->
              <ol class="paper-list">
                <li>
                  Pooja Guhan, Uttaran Bhattacharya, Somdeb Sarkhel, Vahid Azizi, Xiang Chen, Saayan Mitra, Aniket Bera, and Dinesh Manocha.
                  <em>
                    "TAME-RD: Text Assisted Replication of Image Multi-Adjustments for Reverse Designing".
                  </em>
                  The 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024.
                  <br>
                  <span class="descriptor poster">Findings</span>
                  <span class="pub-link">
                    <a href="https://aclanthology.org/2024.findings-acl.637/" target="_blank">
                      Paper
                    </a>
                  </span>
                  <span class="pub-link">
                    <a href="https://umd.box.com/s/rbslp85u3q9738934sm5sjhlh9xmy3zr" target="_blank">
                      Dataset
                    </a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xx9nrfoAAAAJ&cstart=20&pagesize=80&citation_for_view=xx9nrfoAAAAJ:blknAaTinKkC"
                        target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita Dasgupta, Saayan Mitra, and Minh Hoai.
                  <em>
                    "HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances".
                  </em>
                  IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.
                  <br>
                  <span class="descriptor poster">Conference Poster</span>
                  <span class="pub-link">
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Narasimhaswamy_HanDiffuser_Text-to-Image_Generation_With_Realistic_Hand_Appearances_CVPR_2024_paper.pdf" target="_blank">
                      Paper
                    </a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xx9nrfoAAAAJ&pagesize=80&citation_for_view=xx9nrfoAAAAJ:bEWYMUwI8FkC"
                        target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman K Singla, Somesh Singh, Uttaran Bhattacharya,
                  Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, and Balaji Krishnamurthy.
                  <em>
                    "Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior".
                  </em>
                  International Conference on Learning Representations (ICLR), 2024.
                  <br>
                  <span class="descriptor spotlight">Conference Spotlight</span>
                  <span class="pub-link"><a href="https://arxiv.org/pdf/2309.00359.pdf" target="_blank">Paper</a></span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xx9nrfoAAAAJ&sortby=pubdate&citation_for_view=xx9nrfoAAAAJ:HDshCWvjkbEC"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Aneesh Bhattacharya, Manas Paranjape, Uttaran Bhattacharya, and Aniket Bera.
                  <em>
                    "DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning".
                  </em>
                  Association for the Advancement of Artificial Intelligence (AAAI), 2024.
                  <br>
                  <span class="descriptor poster">Conference Poster</span>
                  <span class="pub-link"><a href="https://arxiv.org/pdf/2303.03870.pdf" target="_blank">Paper</a></span>
                  <span class="pub-link"><a href="https://github.com/aneeshbhattacharya/DanceAnyWay" target="_blank">Code</a></span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xx9nrfoAAAAJ&sortby=pubdate&citation_for_view=xx9nrfoAAAAJ:hC7cP41nSMkC"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
              </ol>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="section-title">
                <h4>Affective Computing</h4>
              </div>
      
              <ol class="paper-list">
                <li>
                  Tanmay Randhavane, Uttaran Bhattacharya, Pooja Kabra, Kyra Kapsaskis, Kurt Gray, Dinesh Manocha, and Aniket Bera.
                  <em>
                    "Learning Gait Emotions Using Affective and Deep Features".
                  </em>
                  ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG), 2022.
                  <br>
                  <span class="descriptor oral">Conference Oral</span><span class="descriptor recognition">Best Paper</span>
                  <span class="pub-link"><a href="https://dl.acm.org/doi/pdf/10.1145/3561975.3562957?casa_token=oupo4XycfXYAAAAA:EfGtJoC4FkodPoZDQS5jLr7orR8Q38NOhSShDvjjHPSx2-xSvUqIx2xvQnvYsMby6w24Bk_Qj7oz" target="_blank">Paper</a></span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Learning+Gait+Emotions+Using+Affective+and+Deep+Features&btnG="
                        target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Uttaran Bhattacharya, Gang Wu, Stefano Petrangeli, Viswanathan Swaminathan, and Dinesh Manocha.
                  <em>
                    "HighlightMe: Detecting Highlights from Human-Centric Videos".
                  </em>
                  IEEE/CVF International Conference on Computer Vision (ICCV), 2021.
                  <br>
                  <span class="descriptor poster">Conference Poster</span>
                  <span class="pub-link"><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Bhattacharya_HighlightMe_Detecting_Highlights_From_Human-Centric_Videos_ICCV_2021_paper.pdf" target="_blank">Paper</a></span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=5729767412526734980&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Uttaran Bhattacharya, Elizabeth Childs, Nicholas Rewkowski, and Dinesh Manocha.
                  <em>
                    "Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective
                    Expression Learning".
                  </em>
                  ACM International Conference on Multimedia (ACMMM), 2021.
                  <br>
                  <span class="descriptor oral">Conference Oral</span><span class="descriptor recognition">Best Paper Nominee</span>
                  <span class="pub-link"><a href="https://arxiv.org/pdf/2108.00262.pdf" target="_blank">Paper</a></span>
                  <span class="pub-link"><a href="https://gamma.umd.edu/s2ag/" target="_blank">Project</a></span>
                  <span class="pub-link">
                    <a href="https://github.com/UttaranB127/speech2affective_gestures" target="_blank">Code</a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=15589768151184005786&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha.
                  <em>
                    "Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents".
                  </em>
                  IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR), 2021.
                  <br>
                  <span class="descriptor oral">Conference Oral</span><span class="descriptor recognition">Best Paper</span>
                  <span class="pub-link"><a href="https://arxiv.org/pdf/2101.11101.pdf" target="_blank">Paper</a></span>
                  <span class="pub-link"><a href="https://gamma.umd.edu/t2g/" target="_blank">Project</a></span>
                  <span class="pub-link">
                    <a href="https://github.com/UttaranB127/Text2Gestures" target="_blank">Code</a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=17545275560376256755&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha.
                  <em> "STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits"</em>.
                  Association for the Advancement of Artificial Intelligence (AAAI), 2020.
                  <br>
                  <span class="descriptor spotlight">Conference Spotlight</span>
                  <span class="pub-link">
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5490/5346" target="_blank">Paper
                    </a>
                  </span>
                  <span class="pub-link"><a href="https://gamma.umd.edu/step/" target="_blank">Project</a></span>
                  <span class="pub-link">
                    <a href="https://github.com/UttaranB127/STEP" target="_blank">Code</a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=12116424974796624637&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
              </ol>
            </div>

            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <div class="section-title">
                <h4>3D Scene Reconstruction</h4>
              </div>
      
              <ol class="paper-list">
                <li>
                  Uttaran Bhattacharya, and Venu Madhav Govindu.
                  <em>"Efficient and Robust Registration on The 3D Special Euclidean Group"</em>.
                  IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
                  <br>
                  <span class="descriptor poster">Conference Poster</span>
                  <span class="pub-link">
                    <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.pdf"
                       target="_blank">Paper
                    </a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=14143849740671103660&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
                <li>
                  Uttaran Bhattacharya, Sumit Veerawal, and Venu Madhav Govindu.
                  <em>"Fast Multiview Registration of 3D Scans using Planar Structures"</em>.
                  International Conference on 3D Vision (3DV), 2017.
                  <br>
                  <span class="descriptor spotlight">Conference Spotlight</span>
                  <span class="pub-link">
                    <a href="http://www.ee.iisc.ac.in/labs/cvl/papers/Uttaran2017FMR.pdf" target="_blank">Paper</a>
                  </span>
                  <span class="pub-link">
                    <a href="https://scholar.google.com/scholar?cluster=11048047513294764458&hl=en&as_sdt=0,5"
                       target="_blank">
                      Google Scholar
                    </a>
                  </span>
                </li>
              </ol>
            </div>
          </div>
        </div>
      </section><!-- End Refereed Publications Section -->

      <!-- ======= Preprints Section ======= -->
      <section id="preprints" class="bodysec">
        <div class="container">

          <div class="section-title">
            <h3>Preprints</h3>
          </div>

          <div class="row">
            <div class="col-lg-12 col-md-12 icon-box" data-aos="fade-up">
              <ol reversed class="paper-list">
                <li>
                  Yizhou Wang, Ruiyi Zhang, Haoliang Wang, Uttaran Bhattacharya, Yun Fu, and Gang Wu.
                  <a href="https://arxiv.org/pdf/2312.02310.pdf" target="_blank">"VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding"</a>.
                </li>
              </ol>
            </div>
          </div>
        </div>
      </section>
      <!-- End Preprints Section -->
    </section>
    <!-- End Bibliography Section -->

    <!-- ======= Contact Section ======= -->
    <section id="contact" class="facts">
      <div class="container">

        <div class="section-title">
          <h2>Contact</h2>
        </div>

        <div class="row no-gutters">
          <div class="col-lg-6 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
            <div class="count-box">
              <i class="bi bi-envelope"></i>
              <p>
                &lt; first 4 letters of name &gt;005 [AT] &lt; Google Mail &gt;
              </p>
            </div>
          </div>

          <div class="col-lg-6 col-md-6 d-md-flex align-items-md-stretch" data-aos="fade-up">
            <div class="count-box">
              <i class="bi bi-geo-alt"></i>
              <p>
                San Jose, CA, USA
              </p>
            </div>
          </div>
        </div>
      </div>
    </section><!-- End Contact Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright
        <strong>
          <a href="https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/" target="_blank">iPortfolio</a>
        </strong>
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/ -->
        Designed by <a href="https://bootstrapmade.com/" target="_blank">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>
  <script src="assets/js/pageload.js"></script>

  <!-- Additional JS File -->
<!--  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>-->
  <script src="https://code.jquery.com/jquery-3.6.0.js"
          integrity="sha256-H+K7U5CnXl1h5ywQfKtSj8PCmoN9aaq30gDh27Xc0jk=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.8.3/modernizr.js"></script>

</body>

</html>
